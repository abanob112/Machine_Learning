{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35roXDEMudbw"
   },
   "source": [
    "# GUC Clustering Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIiItKbYudb2"
   },
   "source": [
    "**Objective:** \n",
    "The objective of this project teach students how to apply clustering to real data sets\n",
    "\n",
    "The projects aims to teach student: \n",
    "* Which clustering approach to use\n",
    "* Compare between Kmeans, Hierarchal, DBScan, and Gaussian Mixtures  \n",
    "* How to tune the parameters of each data approach\n",
    "* What is the effect of different distance functions (optional) \n",
    "* How to evaluate clustering approachs \n",
    "* How to display the output\n",
    "* What is the effect of normalizing the data \n",
    "\n",
    "Students in this project will use ready-made functions from Sklearn, plotnine, numpy and pandas \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtHElDYdudb3"
   },
   "outputs": [],
   "source": [
    "# if plotnine is not installed in Jupter then use the following command to install it \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RHS5ZoQudb4"
   },
   "source": [
    "Running this project require the following imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrueqJenudb5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as prep\n",
    "from sklearn.datasets import make_blobs\n",
    "from plotnine import *   \n",
    "# StandardScaler is a function to normalize the data \n",
    "# You may also check MinMaxScaler and MaxAbsScaler \n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This if i want to cluster to the maximum of 7 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ju2Zj6-nudb5"
   },
   "outputs": [],
   "source": [
    "# Helper function that allows us to display data in 2 dimensions and highlights the clusters\n",
    "#def display_cluster(X, clustering_model=None, num_clusters=0):\n",
    " #   color = 'brgcmyk'  # List colors\n",
    "  #  alpha = 0.5  # Color opacity\n",
    "   # s = 20\n",
    "\n",
    "    #if clustering_model is None or not hasattr(clustering_model, 'labels_'):\n",
    "     #   plt.scatter(X[:, 0], X[:, 1], c=color[0], alpha=alpha, s=s)\n",
    "    #else:\n",
    "     #   for i in range(num_clusters):\n",
    "      #      plt.scatter(X[clustering_model.labels_ == i, 0], X[clustering_model.labels_ == i, 1],\n",
    "       #                 c=color[i % len(color)], alpha=alpha, s=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This if i want to cluster more than 7 clusters with different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cluster(X, clustering_model=None, num_clusters=0):\n",
    "    alpha = 0.5  # Color opacity\n",
    "    s = 20\n",
    "\n",
    "    if clustering_model is None or not hasattr(clustering_model, 'labels_'):\n",
    "        plt.scatter(X[:, 0], X[:, 1], alpha=alpha, s=s)\n",
    "    else:\n",
    "        unique_labels = np.unique(clustering_model.labels_)\n",
    "        colors = plt.cm.get_cmap('tab10', len(unique_labels))  # Use a color map for better distinction\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            plt.scatter(X[clustering_model.labels_ == label, 0], X[clustering_model.labels_ == label, 1],\n",
    "                        color=colors(i), alpha=alpha, s=s)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZnIbT3Mudb6"
   },
   "source": [
    "## Multi Blob Data Set \n",
    "* The Data Set generated below has 6 cluster with varying number of users and varing densities\n",
    "* Cluster the data set below using \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeSqG318udb7",
    "outputId": "078fad92-3073-4558-b1e8-f0acd8d85d34"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [8,8]\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "n_bins = 6  \n",
    "centers = [(-3, -3), (0, 0), (5,2.5),(-1, 4), (4, 6), (9,7)]\n",
    "Multi_blob_Data, y = make_blobs(n_samples=[100,150, 300, 400,300, 200], n_features=2, cluster_std=[1.3,0.6, 1.2, 1.7,0.9,1.7],\n",
    "                  centers=centers, shuffle=False, random_state=42)\n",
    "display_cluster(Multi_blob_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDSIGjubudb8"
   },
   "source": [
    "### Kmeans \n",
    "* Use Kmeans with different values of K to cluster the above data \n",
    "* Display the outcome of each value of K \n",
    "* Plot distortion function versus K and choose the approriate value of k \n",
    "* Plot the silhouette_score versus K and use it to choose the best K \n",
    "* Store the silhouette_score for the best K for later comparison with other clustering techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Ne3KmtPudb9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Display the original data\n",
    "plt.figure(figsize=(10, 6))\n",
    "display_cluster(Multi_blob_Data)\n",
    "plt.title('Original Data')\n",
    "plt.show()\n",
    "\n",
    "# Apply KMeans with different values of K\n",
    "k_values = [2, 3, 4, 5, 6,7,8,9,10,11]\n",
    "silhouette_scores = []\n",
    "distortion_values = []  # List to store distortion values\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    Multi_blob_Data_labels = kmeans.fit_predict(Multi_blob_Data)\n",
    "    \n",
    "    # Display the outcome of each value of K\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    display_cluster(Multi_blob_Data, kmeans, num_clusters=k)\n",
    "    plt.title(f'KMeans Clustering with K={k}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot distortion function versus K\n",
    "    distortion = kmeans.inertia_\n",
    "    distortion_values.append(distortion)\n",
    "\n",
    "    # Plot silhouette_score versus K\n",
    "    silhouette = silhouette_score(Multi_blob_Data, Multi_blob_Data_labels)\n",
    "    silhouette_scores.append(silhouette)\n",
    "\n",
    "# Plot distortion function versus K\n",
    "#plt.plot(k_values, distortion_values, marker='o', linestyle='-', color='b')\n",
    "#plt.xlabel('Number of Clusters (K)')\n",
    "#plt.ylabel('Distortion')\n",
    "#plt.title('Elbow Method for Optimal K')\n",
    "#plt.show()\n",
    "\n",
    "# Plot silhouette_score versus K\n",
    "#plt.plot(k_values, silhouette_scores, marker='o', linestyle='-', color='g')\n",
    "#plt.xlabel('Number of Clusters (K)')\n",
    "#plt.ylabel('Silhouette Score')\n",
    "#plt.title('Silhouette Score for Optimal K')\n",
    "#plt.show()\n",
    "\n",
    "# Choose the best K based on the silhouette score\n",
    "best_k = k_values[np.argmax(silhouette_scores)]\n",
    "print(f'Best K based on Silhouette Score: {best_k}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_k(data, k_values):\n",
    "    silhouette_scores = []\n",
    "    distortion_values = []  # List to store distortion values\n",
    "\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        data_labels = kmeans.fit_predict(data)\n",
    "\n",
    "        # Display the outcome of each value of K\n",
    "        #plt.figure(figsize=(10, 6))\n",
    "        #display_cluster(data, kmeans, num_clusters=k)\n",
    "        #plt.title(f'KMeans Clustering with K={k}')\n",
    "        #plt.show()\n",
    "\n",
    "        # Plot distortion function versus K\n",
    "        distortion = kmeans.inertia_\n",
    "        distortion_values.append(distortion)\n",
    "\n",
    "        # Plot silhouette_score versus K\n",
    "        silhouette = silhouette_score(data, data_labels)\n",
    "        silhouette_scores.append(silhouette)\n",
    "\n",
    "    # Plot distortion function versus K\n",
    "    #plt.plot(k_values, distortion_values, marker='o', linestyle='-', color='b')\n",
    "    #plt.xlabel('Number of Clusters (K)')\n",
    "    #plt.ylabel('Distortion')\n",
    "    #plt.title('Elbow Method for Optimal K')\n",
    "    #plt.show()\n",
    "\n",
    "    # Plot silhouette_score versus K\n",
    "    #plt.plot(k_values, silhouette_scores, marker='o', linestyle='-', color='g')\n",
    "    #plt.xlabel('Number of Clusters (K)')\n",
    "    #plt.ylabel('Silhouette Score')\n",
    "    #plt.title('Silhouette Score for Optimal K')\n",
    "    #plt.show()\n",
    "\n",
    "    # Choose the best K based on the silhouette score\n",
    "    best_k = k_values[np.argmax(silhouette_scores)]\n",
    "    print(f'Best K based on Silhouette Score: {best_k}')\n",
    "\n",
    "# Display the original data\n",
    "#plt.figure(figsize=(10, 6))\n",
    "display_cluster(Multi_blob_Data)\n",
    "#plt.title('Original Data')\n",
    "#plt.show()\n",
    "\n",
    "# Apply KMeans with different values of K\n",
    "k_values = [2, 3, 4, 5, 6, 7,8,9,10,11]  # Extend the list of K values as needed\n",
    "find_optimal_k(Multi_blob_Data, k_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE7dvpOAudb9"
   },
   "source": [
    "### Hierarchal Clustering\n",
    "* Use AgglomerativeClustering function to  to cluster the above data \n",
    "* In the  AgglomerativeClustering change the following parameters \n",
    "    * Affinity (use euclidean, manhattan and cosine)\n",
    "    * Linkage( use average and single )\n",
    "    * Distance_threshold (try different)\n",
    "* For each of these trials plot the Dendograph , calculate the silhouette_score and display the resulting clusters  \n",
    "* Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. \n",
    "* Record your observation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3O_6WwKoudb-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Helper function to display dendrogram\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    linkage_matrix = linkage(model.children_, model.linkage)\n",
    "    \n",
    "    #plt.figure(figsize=(15, 6))\n",
    "    dendrogram(linkage_matrix, orientation='top', **kwargs)\n",
    "    \n",
    "    # Add a horizontal line for the distance threshold\n",
    "    color_threshold = kwargs.get('color_threshold',None)\n",
    "    if color_threshold is not None:\n",
    "        #plt.axhline(y=color_threshold, color='r', linestyle='--', label='Distance Threshold')\n",
    "        #plt.legend()\n",
    "\n",
    "\n",
    "# Generate synthetic data\n",
    "n_bins = 6\n",
    "centers = [(-3, -3), (0, 0), (5, 2.5), (-1, 4), (4, 6), (9, 7)]\n",
    "Multi_blob_Data, y = make_blobs(n_samples=[100, 150, 300, 400, 300, 200], n_features=2,\n",
    "                                cluster_std=[1.3, 0.6, 1.2, 1.7, 0.9, 1.7], centers=centers, shuffle=False, random_state=42)\n",
    "\n",
    "# Display the original data\n",
    "#plt.figure(figsize=(10, 6))\n",
    "display_cluster(Multi_blob_Data)\n",
    "#plt.title('Original Data')\n",
    "#plt.show()\n",
    "\n",
    "# AgglomerativeClustering with different parameters\n",
    "affinities = ['euclidean', 'manhattan', 'cosine']\n",
    "linkages = ['average', 'single']\n",
    "distance_thresholds = [None,900,900]\n",
    "\n",
    "best_silhouette_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "for affinity in affinities:\n",
    "    for linkage_type in linkages:\n",
    "        for distance_threshold in distance_thresholds:\n",
    "            # Apply AgglomerativeClustering\n",
    "            if distance_threshold is not None:\n",
    "                agg_clustering = AgglomerativeClustering(n_clusters=None, affinity=affinity, linkage=linkage_type,\n",
    "                                                         distance_threshold=distance_threshold)\n",
    "            else:\n",
    "                n_clusters = 3  # Set n_clusters to the desired number\n",
    "                agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, affinity=affinity, linkage=linkage_type)\n",
    "\n",
    "            Multi_blob_Data_labels = agg_clustering.fit_predict(Multi_blob_Data)\n",
    "\n",
    "            # Plot dendrogram\n",
    "            #plt.figure(figsize=(15, 6))\n",
    "            #plot_dendrogram(agg_clustering, truncate_mode='level', p=3, color_threshold=distance_threshold)\n",
    "            #plt.title(f'Dendrogram - Affinity: {affinity}, Linkage: {linkage_type}, Distance Threshold: {distance_threshold}')\n",
    "            #plt.show()\n",
    "\n",
    "            # Display the resulting clusters\n",
    "            #plt.figure(figsize=(10, 6))\n",
    "            #display_cluster(Multi_blob_Data, agg_clustering, num_clusters=len(set(Multi_blob_Data_labels)))\n",
    "            #plt.title(f'Agglomerative Clustering - Affinity: {affinity}, Linkage: {linkage_type}, Distance Threshold: {distance_threshold}')\n",
    "            #plt.show()\n",
    "\n",
    "            # Calculate silhouette score only if there are more than 1 cluster\n",
    "            if len(set(Multi_blob_Data_labels)) > 1:\n",
    "                silhouette = silhouette_score(Multi_blob_Data, Multi_blob_Data_labels)\n",
    "                print(f'Silhouette Score: {silhouette}')\n",
    "            else:\n",
    "                print(\"Not enough clusters for silhouette score calculation.\")\n",
    "\n",
    "            # Store the best parameters and silhouette score\n",
    "            if silhouette > best_silhouette_score:\n",
    "                best_silhouette_score = silhouette\n",
    "                best_params = {'Affinity': affinity, 'Linkage': linkage_type, 'Distance Threshold': distance_threshold}\n",
    "\n",
    "# Display the best parameters and silhouette score\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Silhouette Score: {best_silhouette_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myJE7vQKudb-"
   },
   "source": [
    "### DBScan\n",
    "* Use DBScan function to  to cluster the above data \n",
    "* In the  DBscan change the following parameters \n",
    "    * EPS (from 0.1 to 3)\n",
    "    * Min_samples (from 5 to 25)\n",
    "* Plot the silhouette_score versus the variation in the EPS and the min_samples\n",
    "* Plot the resulting Clusters in this case \n",
    "* Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. \n",
    "* Record your observations and comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiQtpAt5udb_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data\n",
    "n_bins = 6\n",
    "centers = [(-3, -3), (0, 0), (5, 2.5), (-1, 4), (4, 6), (9, 7)]\n",
    "Multi_blob_Data, y = make_blobs(n_samples=[100, 150, 300, 400, 300, 200], n_features=2,\n",
    "                                cluster_std=[1.3, 0.6, 1.2, 1.7, 0.9, 1.7], centers=centers, shuffle=False, random_state=42)\n",
    "\n",
    "# Helper function to display clusters for DBScan\n",
    "def display_dbscan_clusters(X, dbscan_model):\n",
    "    unique_labels = set(dbscan_model.labels_)\n",
    "    num_clusters = len(unique_labels) - (1 if -1 in dbscan_model.labels_ else 0)\n",
    "\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        if label == -1:\n",
    "            #plt.scatter(X[dbscan_model.labels_ == label, 0], X[dbscan_model.labels_ == label, 1],\n",
    "                        c='gray', alpha=0.5, s=20, label='Noise')\n",
    "        else:\n",
    "            #plt.scatter(X[dbscan_model.labels_ == label, 0], X[dbscan_model.labels_ == label, 1],\n",
    "                        alpha=0.5, s=20, label=f'Cluster {i}')\n",
    "\n",
    "    #plt.legend()\n",
    "    #plt.title(f'DBScan Clustering - EPS: {dbscan_model.eps}, Min Samples: {dbscan_model.min_samples}')\n",
    "    #plt.show()\n",
    "\n",
    "# Parameters to try\n",
    "eps_values = np.arange(0.1, 3.1, 0.5)\n",
    "min_samples_values = range(5, 26)\n",
    "\n",
    "best_silhouette_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "\n",
    "# Loop over parameter combinations\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan_labels = dbscan_model.fit_predict(Multi_blob_Data)\n",
    "        \n",
    "        # Ignore cases where there is only one cluster\n",
    "        if len(set(dbscan_labels)) > 1:\n",
    "            silhouette = silhouette_score(Multi_blob_Data, dbscan_labels)\n",
    "            print(f'EPS: {eps}, Min Samples: {min_samples}, Silhouette Score: {silhouette}')\n",
    "\n",
    "            # Update best parameters if a better silhouette score is found\n",
    "            if silhouette > best_silhouette_score:\n",
    "                best_silhouette_score = silhouette\n",
    "                best_params = {'EPS': eps, 'Min Samples': min_samples}\n",
    "\n",
    "            # Display clusters for the current parameters\n",
    "            #plt.figure(figsize=(10, 6))\n",
    "            display_dbscan_clusters(Multi_blob_Data, dbscan_model)\n",
    "\n",
    "# Display the best parameters and silhouette score\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Silhouette Score: {best_silhouette_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Generate synthetic data\n",
    "n_bins = 6\n",
    "centers = [(-3, -3), (0, 0), (5, 2.5), (-1, 4), (4, 6), (9, 7)]\n",
    "Multi_blob_Data, y = make_blobs(n_samples=[100, 150, 300, 400, 300, 200], n_features=2,\n",
    "                                cluster_std=[1.3, 0.6, 1.2, 1.7, 0.9, 1.7], centers=centers, shuffle=False, random_state=42)\n",
    "\n",
    "# Visualize the original data\n",
    "#plt.figure(figsize=(8, 8))\n",
    "#plt.scatter(Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], alpha=0.5, s=20)\n",
    "#plt.title('Original Data')\n",
    "#plt.show()\n",
    "\n",
    "# Convert data to pandas DataFrame\n",
    "df = pd.DataFrame(Multi_blob_Data, columns=['Feature1', 'Feature2'])\n",
    "\n",
    "# Parameters to try\n",
    "eps_values = np.arange(0.1, 3.1, 1)\n",
    "min_samples_values = range(5, 26)\n",
    "\n",
    "best_silhouette_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "# Loop over parameter combinations\n",
    "for eps, min_samples in itertools.product(eps_values, min_samples_values):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    current_clusters = dbscan.fit_predict(df)\n",
    "    unique_labels = np.unique(current_clusters)\n",
    "\n",
    "    if len(unique_labels) > 1:  # Check if more than one cluster is formed\n",
    "        # Display clusters for the current parameters\n",
    "        #plt.figure(figsize=(10, 6))\n",
    "        #plt.scatter(df['Feature1'], df['Feature2'], c=current_clusters, alpha=0.5, s=20)\n",
    "        #plt.title(f'DBScan Clustering - EPS: {eps}, Min Samples: {min_samples}')\n",
    "        #plt.show()\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        valid_labels = current_clusters != -1\n",
    "        silhouette = silhouette_score(df, current_clusters)\n",
    "        print(f'EPS: {eps}, Min Samples: {min_samples}, Silhouette Score: {silhouette}')\n",
    "\n",
    "        # Update best parameters if a better silhouette score is found\n",
    "        if silhouette > best_silhouette_score:\n",
    "            best_silhouette_score = silhouette\n",
    "            best_params = {'EPS': eps, 'Min Samples': min_samples}\n",
    "\n",
    "# Display the best parameters and silhouette score\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Silhouette Score: {best_silhouette_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip16g1QFudb_"
   },
   "source": [
    "### Gaussian Mixture\n",
    "* Use GaussianMixture function to cluster the above data \n",
    "* In GMM change the covariance_type and check the difference in the resulting proabability fit \n",
    "* Use a 2D contour plot to plot the resulting distribution (the components of the GMM) as well as the total Gaussian mixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = [100, 150, 300, 400, 300, 200]\n",
    "centers = [(-3, -3), (0, 0), (5, 2.5), (-1, 4), (4, 6), (9, 7)]\n",
    "X, y = make_blobs(n_samples=n_samples, n_features=2, cluster_std=[1.3, 0.6, 1.2, 1.7, 0.9, 1.7], centers=centers, shuffle=False, random_state=42)\n",
    "\n",
    "# Fit Gaussian Mixture Model with different covariance types\n",
    "covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
    "#plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, cov_type in enumerate(covariance_types, 1):\n",
    "    gmm = GaussianMixture(n_components=len(centers), covariance_type=cov_type, random_state=42)\n",
    "    gmm.fit(X)\n",
    "    \n",
    "    # Plot 2D contour plot for each component\n",
    "    #plt.subplot(2, 2, i)\n",
    "    #plt.scatter(X[:, 0], X[:, 1], c=gmm.predict(X), cmap='viridis', s=20, alpha=0.7)\n",
    "    #plt.title(f'GMM with Covariance Type: {cov_type}')\n",
    "    \n",
    "    # Plot ellipses for each component\n",
    "    for j in range(min(len(gmm.covariances_), len(centers))):\n",
    "        mean = gmm.means_[j]\n",
    "        \n",
    "        if cov_type == 'spherical':\n",
    "            var = gmm.covariances_[j]\n",
    "            covar = np.diag(var * np.ones_like(mean))  # Construct a diagonal covariance matrix\n",
    "        else:\n",
    "            covar = gmm.covariances_[j]\n",
    "            \n",
    "        if len(covar.shape) == 1:\n",
    "            # Handling the case where covar is 1-dimensional (spherical case)\n",
    "            covar = np.diag(covar * np.ones_like(mean))  # Construct a diagonal covariance matrix\n",
    "            \n",
    "        try:\n",
    "            covar = covar.reshape(2, 2)\n",
    "        except ValueError:\n",
    "            # Handling the case where covar is still not 2D (e.g., 'spherical' case)\n",
    "            covar = np.diag(covar * np.ones_like(mean))  # Construct a diagonal covariance matrix\n",
    "            \n",
    "        v, w = np.linalg.eigh(covar)\n",
    "        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)\n",
    "        u = w[0] / np.linalg.norm(w[0])\n",
    "        \n",
    "        angle = np.arctan(u[1] / u[0])\n",
    "        angle = 180.0 + angle * 180.0 / np.pi\n",
    "        ell = patches.Ellipse(mean, v[0], v[1], angle, color='red', alpha=0.5)\n",
    "        plt.gca().add_patch(ell)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m92lZkkyudb_"
   },
   "source": [
    "## iris data set \n",
    "The iris data set is test data set that is part of the Sklearn module \n",
    "which contains 150 records each with 4 features. All the features are represented by real numbers \n",
    "\n",
    "The data represents three classes \n",
    "\n",
    "* Repeat all the above clustering approaches and steps on the above data \n",
    "* Normalize the data then repeat all the above steps \n",
    "* Compare between the different clustering approaches \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QaCWyyCudcA",
    "outputId": "79c14dba-80cf-4d96-e69d-70763b789faf"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()\n",
    "iris_data.target[[10, 25, 50]]\n",
    "#array([0, 0, 1])\n",
    "list(iris_data.target_names)\n",
    "['setosa', 'versicolor', 'virginica']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Load Iris dataset\n",
    "iris_data = load_iris()\n",
    "X_iris = iris_data.data\n",
    "\n",
    "# Create a DataFrame with the dataset and column names\n",
    "iris_df = pd.DataFrame(X_iris, columns=iris_data.feature_names)\n",
    "\n",
    "target = iris_data.target  # Add this line to get the target values\n",
    "\n",
    "# Create a DataFrame with the dataset and column names\n",
    "iris_df = pd.DataFrame(X_iris, columns=iris_data.feature_names)\n",
    "iris_df['target'] = target  # Add this line to include the 'target' column\n",
    "\n",
    "\n",
    "# Print the first few lines of the dataset\n",
    "print(iris_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the first two features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=iris_df['sepal length (cm)'], y=iris_df['sepal width (cm)'], hue=iris_df['target'], palette='viridis', s=70, alpha=0.8)\n",
    "plt.title('Scatter Plot of Iris Dataset (Original)')\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.legend(title='Target', loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This kmean clustering on iris data before normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(X, algorithm, algorithm_name, normalize=False, print_silhouette=True):\n",
    "    # Normalize the data if specified\n",
    "    if normalize:\n",
    "        scaler = StandardScaler()\n",
    "        X_normalized = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X_normalized = X\n",
    "\n",
    "    # Apply the clustering algorithm\n",
    "    clustering_algorithm = algorithm\n",
    "    labels = clustering_algorithm.fit_predict(X_normalized)\n",
    "\n",
    "    # Visualize the clusters using PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_normalized)\n",
    "\n",
    "    #plt.figure(figsize=(10, 6))\n",
    "    #plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', s=50, alpha=0.8)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(X_normalized, labels)\n",
    "    \n",
    "    # Display silhouette score below the plot\n",
    "    #plt.title(f'{algorithm_name} Clustering')\n",
    "    #plt.show()\n",
    "    \n",
    "    print(f'Silhouette Score for {algorithm_name}: {silhouette_avg:.3f}\\n')\n",
    "\n",
    "    return silhouette_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# K-Means Clustering\n",
    "num_clusters_range = range(2, 7)\n",
    "\n",
    "for k in num_clusters_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    perform_clustering(X_iris, kmeans, f'K-Means (k={k})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This agglomerative clustering on iris data before normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display clusters for Agglomerative Clustering\n",
    "def display_agglomerative_clusters(X, linkage_matrix, threshold):\n",
    "    # Cluster the data using the threshold\n",
    "    clustered_labels = fcluster(linkage_matrix, threshold, criterion='distance')\n",
    "\n",
    "    # Display clustered data\n",
    "    #plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot all data points with the same color\n",
    "    #plt.scatter(X[:, 0], X[:, 1], c='gray', alpha=0.5, s=20, label='Noise')\n",
    "\n",
    "    # Plot clustered data points with different colors\n",
    "    unique_labels = np.unique(clustered_labels)\n",
    "    colors = plt.cm.get_cmap('tab10', len(unique_labels))\n",
    "\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        if label != 0:  # Exclude noise points\n",
    "            plt.scatter(X[clustered_labels == label, 0], X[clustered_labels == label, 1],\n",
    "                        color=colors(i), alpha=0.5, s=20, label=f'Cluster {label}')\n",
    "\n",
    "    #plt.title(f'Agglomerative Clustering with Threshold {threshold}')\n",
    "    #plt.xlabel('Feature 1')\n",
    "    #plt.ylabel('Feature 2')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "# Calculate linkage matrix for dendrogram\n",
    "linkage_matrix = linkage(X_iris, method='ward')\n",
    "\n",
    "# Plot dendrogram for all data points\n",
    "#plt.figure(figsize=(12, 8))\n",
    "dendrogram(linkage_matrix, orientation='top', labels=None, distance_sort='descending')\n",
    "\n",
    "# Set a threshold for clustering\n",
    "threshold = 3  # You can adjust this threshold\n",
    "plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold: {threshold}')\n",
    "plt.legend()\n",
    "\n",
    "#plt.title('Agglomerative Clustering Dendrogram with Threshold')\n",
    "#plt.xlabel('Data Points')\n",
    "#plt.ylabel('Distance')\n",
    "#plt.show()\n",
    "\n",
    "# Display clustered data using the threshold\n",
    "display_agglomerative_clusters(X_iris, linkage_matrix, threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This DBSCAN clustering on iris data before normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Clustering\n",
    "eps_values = [0.3, 0.5, 0.8,1]\n",
    "min_samples_values = [3, 5, 7,9]\n",
    "\n",
    "best_silhouette_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        silhouette_avg = perform_clustering(X_iris, dbscan, f'DBSCAN (EPS={eps}, Min Samples={min_samples})', normalize=True, print_silhouette=False)\n",
    "\n",
    "        # Update best parameters if a better silhouette score is found\n",
    "        if silhouette_avg > best_silhouette_score:\n",
    "            best_silhouette_score = silhouette_avg\n",
    "            best_params = {'EPS': eps, 'Min Samples': min_samples}\n",
    "\n",
    "# Display the best parameters and silhouette score\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Silhouette Score: {best_silhouette_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This GMM clustering on iris data before normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Mixture Model (GMM)\n",
    "n_components_range = range(2, 7)\n",
    "\n",
    "for n_components in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n_components)\n",
    "    perform_clustering(X_iris, gmm, f'Gaussian Mixture Model (n_components={n_components})', normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyoCVfyMudcA"
   },
   "source": [
    "## Now repeat all the clustering techniques on the iris data set but after normalizing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  *This KMean clustering on iris data after normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_iris_normalized = scaler.fit_transform(X_iris)\n",
    "\n",
    "# K-Means Clustering on the normalized data\n",
    "num_clusters_range = range(2, 6)\n",
    "\n",
    "for k in num_clusters_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    perform_clustering(X_iris_normalized, kmeans, f'K-Means (k={k})', normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This AGGLOMERATIVE clustering on iris data AFTER normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_iris_normalized = scaler.fit_transform(X_iris)\n",
    "\n",
    "# Helper function to display clusters for Agglomerative Clustering\n",
    "def display_agglomerative_clusters(X, linkage_matrix, threshold):\n",
    "    # Cluster the data using the threshold\n",
    "    clustered_labels = fcluster(linkage_matrix, threshold, criterion='distance')\n",
    "\n",
    "    # Display clustered data\n",
    "    #plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot all data points with the same color\n",
    "    #plt.scatter(X[:, 0], X[:, 1], c='gray', alpha=0.5, s=20, label='Noise')\n",
    "\n",
    "    # Plot clustered data points with different colors\n",
    "    unique_labels = np.unique(clustered_labels)\n",
    "    colors = plt.cm.get_cmap('tab10', len(unique_labels))\n",
    "\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        if label != 0:  # Exclude noise points\n",
    "            #plt.scatter(X[clustered_labels == label, 0], X[clustered_labels == label, 1],\n",
    "                        color=colors(i), alpha=0.5, s=20, label=f'Cluster {label}')\n",
    "\n",
    "    #plt.title(f'Agglomerative Clustering with Threshold {threshold}')\n",
    "    #plt.xlabel('Feature 1')\n",
    "    #plt.ylabel('Feature 2')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "# Calculate linkage matrix for dendrogram\n",
    "linkage_matrix = linkage(X_iris_normalized, method='ward')\n",
    "\n",
    "# Plot dendrogram for all data points\n",
    "#plt.figure(figsize=(12, 8))\n",
    "dendrogram(linkage_matrix, orientation='top', labels=None, distance_sort='descending')\n",
    "\n",
    "# Set a threshold for clustering\n",
    "threshold = 3  # You can adjust this threshold\n",
    "#plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold: {threshold}')\n",
    "#plt.legend()\n",
    "\n",
    "#plt.title('Agglomerative Clustering Dendrogram with Threshold')\n",
    "#plt.xlabel('Data Points')\n",
    "#plt.ylabel('Distance')\n",
    "#plt.show()\n",
    "\n",
    "# Display clustered data using the threshold\n",
    "display_agglomerative_clusters(X_iris_normalized, linkage_matrix, threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This DBSCAN clustering on iris data AFTER normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_iris_normalized = scaler.fit_transform(X_iris)\n",
    "\n",
    "# DBSCAN Clustering\n",
    "eps_values = [0.3, 0.5, 0.8]\n",
    "min_samples_values = [3, 5, 7]\n",
    "\n",
    "best_silhouette_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        silhouette_avg = perform_clustering(X_iris_normalized, dbscan, f'DBSCAN (EPS={eps}, Min Samples={min_samples})', normalize=False, print_silhouette=False)\n",
    "\n",
    "        # Update best parameters if a better silhouette score is found\n",
    "        if silhouette_avg > best_silhouette_score:\n",
    "            best_silhouette_score = silhouette_avg\n",
    "            best_params = {'EPS': eps, 'Min Samples': min_samples}\n",
    "\n",
    "# Display the best parameters and silhouette score\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Silhouette Score: {best_silhouette_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This GMM clustering on iris data AFTER normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_iris_normalized = scaler.fit_transform(X_iris)\n",
    "\n",
    "# Gaussian Mixture Model (GMM) Clustering\n",
    "n_components_range = range(2, 6)\n",
    "\n",
    "for n_components in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n_components)\n",
    "    perform_clustering(X_iris_normalized, gmm, f'Gaussian Mixture Model (n_components={n_components})', normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2oBmWT2udcA"
   },
   "source": [
    "## Customer dataset\n",
    "Repeat all the above on the customer data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read the customer dataset from the CSV file\n",
    "customer_data = pd.read_csv('/Users/maria/Desktop/semster10/ML/ASSIGNM/1/Customer data.csv')\n",
    "\n",
    "# Extract the feature columns (assuming the first column is the target variable)\n",
    "X_customer = customer_data.iloc[:, 1:]\n",
    "\n",
    "# Apply PCA to reduce dimensionality to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_customer_pca = pca.fit_transform(X_customer)\n",
    "\n",
    "# Visualize the original data using PCA\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#sns.scatterplot(x=X_customer_pca[:, 0], y=X_customer_pca[:, 1])\n",
    "#plt.title('Customer Data - PCA')\n",
    "#plt.xlabel('Principal Component 1')\n",
    "#plt.ylabel('Principal Component 2')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_customer_std = scaler.fit_transform(X_customer)\n",
    "\n",
    "# KMeans clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_customer_std)\n",
    "\n",
    "# Visualize KMeans clustering results\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#sns.scatterplot(x=X_customer_pca[:, 0], y=X_customer_pca[:, 1], hue=kmeans_labels, palette='viridis', legend='full')\n",
    "#plt.title('KMeans Clustering Results (Customer Data)')\n",
    "#plt.xlabel('Principal Component 1')\n",
    "#plt.ylabel('Principal Component 2')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Agglomerative clustering\n",
    "agglomerative = AgglomerativeClustering(n_clusters=3)\n",
    "agglomerative_labels = agglomerative.fit_predict(X_customer_std)\n",
    "\n",
    "# Visualize Agglomerative clustering results\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#sns.scatterplot(x=X_customer_pca[:, 0], y=X_customer_pca[:, 1], hue=agglomerative_labels, palette='viridis', legend='full')\n",
    "#plt.title('Agglomerative Clustering Results (Customer Data)')\n",
    "#plt.xlabel('Principal Component 1')\n",
    "#plt.ylabel('Principal Component 2')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=3, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_customer_std)\n",
    "\n",
    "# Visualize DBSCAN clustering results\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#sns.scatterplot(x=X_customer_pca[:, 0], y=X_customer_pca[:, 1], hue=dbscan_labels, palette='viridis', legend='full')\n",
    "#plt.title('DBSCAN Clustering Results (Customer Data)')\n",
    "#plt.xlabel('Principal Component 1')\n",
    "#plt.ylabel('Principal Component 2')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load customer data\n",
    "customer_data_path = '/Users/maria/Desktop/semster10/ML/ASSIGNM/1/Customer data.csv'\n",
    "customer_data = pd.read_csv(customer_data_path)\n",
    "\n",
    "# Display the first few rows of the dataset to identify the correct column names\n",
    "print(customer_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This KMEAN clustering on customer data before normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your customer data (replace 'your_data.csv' with the actual path)\n",
    "customer_data = pd.read_csv('/Users/maria/Desktop/semster10/ML/ASSIGNM/1/Customer data.csv')\n",
    "\n",
    "# Specify the features for clustering (replace 'feature1' and 'feature2' with actual column names)\n",
    "feature1 = 'Age'\n",
    "feature2 = 'Income'\n",
    "\n",
    "# Extract the selected features\n",
    "X_customer = customer_data[[feature1, feature2]]\n",
    "\n",
    "# Visualize the original data\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#sns.scatterplot(x=feature1, y=feature2, data=customer_data, s=70, alpha=0.8)\n",
    "#plt.title('Scatter Plot of Customer Data (Original)')\n",
    "#plt.xlabel(feature1)\n",
    "#plt.ylabel(feature2)\n",
    "#plt.show()\n",
    "\n",
    "# Specify the range of clusters\n",
    "num_clusters_range = range(2, 10)\n",
    "\n",
    "# Iterate over the cluster range\n",
    "for k in num_clusters_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    perform_clustering(X_customer, kmeans, f'K-Means (k={k})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This AGGLOMERATIVE clustering on customer data before normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Specify the features for clustering\n",
    "feature1 = 'Age'\n",
    "feature2 = 'Income'\n",
    "\n",
    "# Extract the selected features\n",
    "X_customer = customer_data[[feature1, feature2]]\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_customer_normalized = scaler.fit_transform(X_customer)\n",
    "\n",
    "# Helper function to display clusters for Agglomerative Clustering\n",
    "def display_agglomerative_clusters(X, linkage_matrix, threshold):\n",
    "    # Cluster the data using the threshold\n",
    "    clustered_labels = fcluster(linkage_matrix, threshold, criterion='distance')\n",
    "\n",
    "    # Display clustered data\n",
    "    #plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot all data points with the same color\n",
    "    #plt.scatter(X[:, 0], X[:, 1], c='gray', alpha=0.5, s=20, label='Noise')\n",
    "\n",
    "    # Plot clustered data points with different colors\n",
    "    unique_labels = np.unique(clustered_labels)\n",
    "    colors = plt.cm.get_cmap('tab10', len(unique_labels))\n",
    "\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        if label != 0:  # Exclude noise points\n",
    "            plt.scatter(X[clustered_labels == label, 0], X[clustered_labels == label, 1],\n",
    "                        color=colors(i), alpha=0.5, s=20, label=f'Cluster {label}')\n",
    "\n",
    "    #plt.title(f'Agglomerative Clustering with Threshold {threshold}')\n",
    "    #plt.xlabel(feature1)\n",
    "    #plt.ylabel(feature2)\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "# Calculate linkage matrix for dendrogram\n",
    "linkage_matrix = linkage(X_customer_normalized, method='ward')\n",
    "\n",
    "# Plot dendrogram for all data points\n",
    "#plt.figure(figsize=(12, 8))\n",
    "dendrogram(linkage_matrix, orientation='top', labels=None, distance_sort='descending')\n",
    "\n",
    "# Set a threshold for clustering\n",
    "threshold = 40  # You can adjust this threshold\n",
    "#plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold: {threshold}')\n",
    "#plt.legend()\n",
    "\n",
    "#plt.title('Agglomerative Clustering Dendrogram with Threshold')\n",
    "#plt.xlabel('Data Points')\n",
    "#plt.ylabel('Distance')\n",
    "#plt.show()\n",
    "\n",
    "# Display clustered data using the threshold\n",
    "display_agglomerative_clusters(X_customer_normalized, linkage_matrix, threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This DBSCAN clustering on customer data before normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 'Age' feature for clustering\n",
    "X_customer_age = customer_data[['Age']].values\n",
    "\n",
    "# Parameters to try\n",
    "eps_values = np.arange(0.1, 3.1, 0.5)\n",
    "min_samples_values = range(5, 26)\n",
    "\n",
    "best_silhouette_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "# Loop over parameter combinations\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan_labels = dbscan_model.fit_predict(X_customer_age)\n",
    "        \n",
    "        # Ignore cases where there is only one cluster\n",
    "        if len(set(dbscan_labels)) > 1:\n",
    "            silhouette = silhouette_score(X_customer_age, dbscan_labels)\n",
    "            print(f'EPS: {eps}, Min Samples: {min_samples}, Silhouette Score: {silhouette}')\n",
    "\n",
    "            # Update best parameters if a better silhouette score is found\n",
    "            if silhouette > best_silhouette_score:\n",
    "                best_silhouette_score = silhouette\n",
    "                best_params = {'EPS': eps, 'Min Samples': min_samples}\n",
    "\n",
    "            # Display clusters for the current parameters\n",
    "            #plt.figure(figsize=(10, 6))\n",
    "            #plt.scatter(X_customer_age[:, 0], np.zeros_like(X_customer_age), c=dbscan_labels, cmap='viridis', s=50, alpha=0.8)\n",
    "            #plt.title(f'DBScan Clustering - EPS: {eps}, Min Samples: {min_samples}')\n",
    "            #plt.show()\n",
    "\n",
    "# Display the best parameters and silhouette score\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Silhouette Score: {best_silhouette_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By using the PCA ON 2 FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This DBSCAN clustering on customer data before normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Select 'Age' and 'Income' features for clustering\n",
    "selected_features = ['Age', 'Income']\n",
    "X_customer_selected = customer_data[selected_features].values\n",
    "\n",
    "# Normalize the data using StandardScaler before PCA\n",
    "scaler = StandardScaler()\n",
    "X_customer_selected_normalized = scaler.fit_transform(X_customer_selected)\n",
    "\n",
    "# Apply PCA to reduce dimensionality to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_customer_pca = pca.fit_transform(X_customer_selected_normalized)\n",
    "\n",
    "# Parameters to try\n",
    "eps_values = np.arange(0.1, 3.1, 0.5)\n",
    "min_samples_values = range(5, 26)\n",
    "\n",
    "best_silhouette_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "# Loop over parameter combinations\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan_labels = dbscan_model.fit_predict(X_customer_pca)\n",
    "        \n",
    "        # Ignore cases where there is only one cluster\n",
    "        if len(set(dbscan_labels)) > 1:\n",
    "            silhouette = silhouette_score(X_customer_pca, dbscan_labels)\n",
    "            print(f'EPS: {eps}, Min Samples: {min_samples}, Silhouette Score: {silhouette}')\n",
    "\n",
    "            # Update best parameters if a better silhouette score is found\n",
    "            if silhouette > best_silhouette_score:\n",
    "                best_silhouette_score = silhouette\n",
    "                best_params = {'EPS': eps, 'Min Samples': min_samples}\n",
    "\n",
    "            # Display clusters for the current parameters\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(X_customer_pca[:, 0], X_customer_pca[:, 1], c=dbscan_labels, cmap='viridis', s=50, alpha=0.8)\n",
    "            plt.title(f'DBScan Clustering with PCA - EPS: {eps}, Min Samples: {min_samples}')\n",
    "            plt.xlabel('Principal Component 1')\n",
    "            plt.ylabel('Principal Component 2')\n",
    "            plt.show()\n",
    "\n",
    "# Display the best parameters and silhouette score\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Silhouette Score: {best_silhouette_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This DBSCAN clustering on customer data before normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select two features for clustering (e.g., 'Age' and 'Income')\n",
    "X_customer_selected = customer_data[['Age', 'Income']].values\n",
    "\n",
    "# GMM Clustering\n",
    "n_components_range = range(2, 9)\n",
    "\n",
    "best_silhouette_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "# Loop over different numbers of components\n",
    "for n_components in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n_components)\n",
    "    gmm_labels = gmm.fit_predict(X_customer_selected)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette = silhouette_score(X_customer_selected, gmm_labels)\n",
    "    print(f'Number of Components: {n_components}, Silhouette Score: {silhouette}')\n",
    "\n",
    "    # Update best parameters if a better silhouette score is found\n",
    "    if silhouette > best_silhouette_score:\n",
    "        best_silhouette_score = silhouette\n",
    "        best_params = {'Number of Components': n_components}\n",
    "\n",
    "    # Display clusters for the current number of components\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_customer_selected[:, 0], X_customer_selected[:, 1], c=gmm_labels, cmap='viridis', s=50, alpha=0.8)\n",
    "    plt.title(f'GMM Clustering - Number of Components: {n_components}')\n",
    "    plt.show()\n",
    "\n",
    "# Display the best parameters and silhouette score\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Silhouette Score: {best_silhouette_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now repeat the same clustering techniques on the normalized data of the customer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This KMEAN clustering on customer data AFTER normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = 'Age'\n",
    "feature2 = 'Income'\n",
    "\n",
    "# Extract the selected features\n",
    "X_customer = customer_data[[feature1, feature2]]\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_customer_normalized = scaler.fit_transform(X_customer)\n",
    "\n",
    "# Visualize the original data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=feature1, y=feature2, data=customer_data, s=70, alpha=0.8)\n",
    "plt.title('Scatter Plot of Customer Data (Original)')\n",
    "plt.xlabel(feature1)\n",
    "plt.ylabel(feature2)\n",
    "plt.show()\n",
    "\n",
    "# Specify the range of clusters\n",
    "num_clusters_range = range(2, 10)\n",
    "\n",
    "# Iterate over the cluster range\n",
    "for k in num_clusters_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    perform_clustering(X_customer_normalized, kmeans, f'K-Means (k={k})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This AGGLOMERATIVE clustering on CUSTOMER data AFTER normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Specify the features for clustering\n",
    "feature1 = 'Age'\n",
    "feature2 = 'Income'\n",
    "\n",
    "# Extract the selected features\n",
    "X_customer = customer_data[[feature1, feature2]]\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_customer_normalized = scaler.fit_transform(X_customer)\n",
    "\n",
    "# Helper function to display clusters for Agglomerative Clustering\n",
    "def display_agglomerative_clusters(X, linkage_matrix, threshold):\n",
    "    # Cluster the data using the threshold\n",
    "    clustered_labels = fcluster(linkage_matrix, threshold, criterion='distance')\n",
    "\n",
    "    # Display clustered data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot all data points with the same color\n",
    "    plt.scatter(X[:, 0], X[:, 1], c='gray', alpha=0.5, s=20, label='Noise')\n",
    "\n",
    "    # Plot clustered data points with different colors\n",
    "    unique_labels = np.unique(clustered_labels)\n",
    "    colors = plt.cm.get_cmap('tab10', len(unique_labels))\n",
    "\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        if label != 0:  # Exclude noise points\n",
    "            plt.scatter(X[clustered_labels == label, 0], X[clustered_labels == label, 1],\n",
    "                        color=colors(i), alpha=0.5, s=20, label=f'Cluster {label}')\n",
    "\n",
    "    plt.title(f'Agglomerative Clustering with Threshold {threshold}')\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(X, clustered_labels)\n",
    "    print(f'Silhouette Score: {silhouette_avg}\\n')\n",
    "\n",
    "# Calculate linkage matrix for dendrogram\n",
    "linkage_matrix = linkage(X_customer_normalized, method='ward')\n",
    "\n",
    "# Plot dendrogram for all data points\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(linkage_matrix, orientation='top', labels=None, distance_sort='descending')\n",
    "\n",
    "# Set a threshold for clustering\n",
    "threshold = 15  # You can adjust this threshold\n",
    "plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold: {threshold}')\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Agglomerative Clustering Dendrogram with Threshold')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Display clustered data using the threshold and calculate silhouette score\n",
    "display_agglomerative_clusters(X_customer_normalized, linkage_matrix, threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This DBSCAN clustering on CUSTOMER data AFTER normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Select 'Age' feature for clustering\n",
    "X_customer_age = customer_data[['Age']].values\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_customer_age_normalized = scaler.fit_transform(X_customer_age)\n",
    "\n",
    "# Parameters to try\n",
    "eps_values = np.arange(0.1, 3.1, 0.5)\n",
    "min_samples_values = range(5, 26)\n",
    "\n",
    "best_silhouette_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "# Loop over parameter combinations\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan_labels = dbscan_model.fit_predict(X_customer_age_normalized)\n",
    "        \n",
    "        # Ignore cases where there is only one cluster\n",
    "        if len(set(dbscan_labels)) > 1:\n",
    "            silhouette = silhouette_score(X_customer_age_normalized, dbscan_labels)\n",
    "            print(f'EPS: {eps}, Min Samples: {min_samples}, Silhouette Score: {silhouette}')\n",
    "\n",
    "            # Update best parameters if a better silhouette score is found\n",
    "            if silhouette > best_silhouette_score:\n",
    "                best_silhouette_score = silhouette\n",
    "                best_params = {'EPS': eps, 'Min Samples': min_samples}\n",
    "\n",
    "            # Display clusters for the current parameters\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(X_customer_age_normalized[:, 0], np.zeros_like(X_customer_age_normalized), c=dbscan_labels, cmap='viridis', s=50, alpha=0.8)\n",
    "            plt.title(f'DBScan Clustering - EPS: {eps}, Min Samples: {min_samples}')\n",
    "            plt.show()\n",
    "\n",
    "# Display the best parameters and silhouette score\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Silhouette Score: {best_silhouette_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *This GMM clustering on CUSTOMER data AFTER normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select two features for clustering (e.g., 'Age' and 'Income')\n",
    "X_customer_selected = customer_data[['Age', 'Income']].values\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_customer_selected_normalized = scaler.fit_transform(X_customer_selected)\n",
    "\n",
    "# GMM Clustering\n",
    "n_components_range = range(2, 9)\n",
    "\n",
    "best_silhouette_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "# Loop over different numbers of components\n",
    "for n_components in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n_components)\n",
    "    gmm_labels = gmm.fit_predict(X_customer_selected_normalized)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette = silhouette_score(X_customer_selected_normalized, gmm_labels)\n",
    "    print(f'Number of Components: {n_components}, Silhouette Score: {silhouette}')\n",
    "\n",
    "    # Update best parameters if a better silhouette score is found\n",
    "    if silhouette > best_silhouette_score:\n",
    "        best_silhouette_score = silhouette\n",
    "        best_params = {'Number of Components': n_components}\n",
    "\n",
    "    # Display clusters for the current number of components\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_customer_selected_normalized[:, 0], X_customer_selected_normalized[:, 1], c=gmm_labels, cmap='viridis', s=50, alpha=0.8)\n",
    "    plt.title(f'GMM Clustering - Number of Components: {n_components}')\n",
    "    plt.show()\n",
    "\n",
    "# Display the best parameters and silhouette score\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Silhouette Score: {best_silhouette_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Clustering Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
